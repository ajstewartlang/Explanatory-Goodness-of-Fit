---
title: "Analysis of Regression Path Reading Times"
author: "Henrik Singmann"
output: 
  html_notebook:
    toc: yes
    toc_float: true
date: "`r format(Sys.time(), '%d %B, %Y')`"
---


```{r setup, results='hide', message=FALSE}
require(dplyr)
require(tidyr)
require(stringr)
require(ggplot2)
require(afex)
require(lattice) # for qqmath
require(gridExtra)
library(ggthemes)
theme_set(theme_light())

```

# Introduction

The goal of the following analysis was to see whether the explanatory goodness of fit would affect the Regression Path (RP) times. However, there were two different variables representing the explanatory goodness of fit: the experimenter assigned binary variable `Fit` with levels 'high' and 'low' and the continuous variable `Accepts` representing how a different group of participants judged the explanatory fit. In addition, we also have a confidence rating for the continuous fit rating, variable `Confident`.
We first compare how these two continuous variables are related to `Fit` (note that fit is identical for all DVs, hence we simply use the first DV).

```{r, fig.width=8, fig.height=7}
RPs_plus_ratings <- read.csv("RPs_plus_ratings.csv")
RPs_plus_ratings$P.s <- factor(RPs_plus_ratings$P.s)
RPs_plus_ratings$Item <- factor(RPs_plus_ratings$Item)

agg1 <- RPs_plus_ratings %>% 
  group_by(Item, Fit) %>% 
  summarise(Accepts = first(Accepts),
            Confident = first(Confident))
agg2 <- RPs_plus_ratings %>% 
  group_by(Item, Fit) %>% 
  summarise(Accepts = mean(Accepts),
            Confident = mean(Confident))

stopifnot(all.equal(agg1, agg2)) # to make sure nothing fishy is going on.
agg1$Fit2 <- jitter(as.numeric(agg1$Fit))

p1 <- ggplot(agg1, aes(x=Fit, y=Accepts)) +
  geom_boxplot(outlier.color = NA) +
  geom_line(aes(x=Fit2, y=Accepts, group = Item), alpha = 0.5) +
  geom_point(aes(x=Fit2, y=Accepts)) +
  stat_summary(fun.y = "mean", colour = "red", size = 2, geom = "point")

p2 <- ggplot(agg1, aes(x=Fit, y=Confident)) +
  geom_boxplot(outlier.color = NA) +
  geom_line(aes(x=Fit2, y=Confident, group = Item), alpha = 0.5) +
  geom_point(aes(x=Fit2, y=Confident)) +
  stat_summary(fun.y = "mean", colour = "red", size = 2, geom = "point")
p3 <- ggplot(agg1, aes(x = Accepts, y = Confident)) +
  geom_point(shape=1) +
  geom_smooth(method=loess) +
  geom_smooth(method=lm, color = "red", se=FALSE)
p4 <- ggplot(agg1, aes(x = Accepts)) +
  geom_histogram(binwidth = 0.25)
p5 <- ggplot(agg1, aes(x = Confident)) +
  geom_histogram(binwidth = 0.25)

grid.arrange(p1, p2, p3, p4, p5, ncol=3)
```

As can be seen the continuous fit rating is in line with the experimenter defined Fit rating. From the graph one question is not entirely clear. Is for any item the judged explanatory fit in the Low condition higher than in the High condition?

```{r}
agg1a <- agg1 %>% 
  select(-Confident, -Fit2) %>% 
  spread(Fit, Accepts)
any(agg1a$High < agg1a$Low)
```

The Confidence rating seems to be less strongly related to fit, but still somehow related to the continuous fit rating (i.e., `Accepts`). We finally check how strong this relationship is.


```{r}
cor.test(~Confident+Accepts, agg1)
```

Given the strong relationship between the two ratings we only consider the `Accepts` rating in the following to avoid multicolinearity. 

```{r, eval=FALSE}
p1_paper <- 
  ggplot(agg1, aes(x=Fit, y=Accepts)) +
  geom_violin(adjust = 1, draw_quantiles = c(0.5)) +
  geom_line(aes(x=Fit2, y=Accepts, group = Item), alpha = 0.5) +
  geom_point(aes(x=Fit2, y=Accepts)) +
  stat_summary(fun.y = "mean", colour = "red", size = 2.5, geom = "point", shape = 17) +
  labs(x = "Explanatory Fit", y = "Participant Ratings")

pdf("fig_fit.pdf", width = 2, height = 2.75)
p1_paper
dev.off()

```


# Data Preparation

```{r}
drp_r <- droplevels(RPs_plus_ratings[RPs_plus_ratings$Item != "9" ,])
nas <- is.na(drp_r$spillover) | is.na(drp_r$Consequent)
drp_r <- droplevels(drp_r[!nas,])

```

For the analysis we remove `r sum(nas)` trials with missing data in either spillover or Consequent. Then we transform all RTs from ms to s and calculate transformed versions using either `log` or the inverse transformation. We also z-transform the continuous `Accepts` variable.

```{r}
drp_r <- drp_r %>%
  mutate(
    antecedent = Antecedent/1000,
    log_ant = log(antecedent),
    inv_ant = 1/antecedent,
    consequent = Consequent/1000,
    log_con = log(consequent),
    inv_con = 1/consequent,
    spillover = spillover/1000,
    log_spill = log(spillover),
    inv_spill = 1/spillover,
    acc_z = (Accepts - mean(Accepts))/sd(Accepts),
    #conf_z = (Confident- mean(Confident))/sd(Confident),
    log_acc= log(Accepts))
```

We then plot the three versions of the DVs.

```{r}
dplot <- drp_r %>% 
  gather("key", "value",consequent:inv_spill, spillover) %>% 
  mutate(region = if_else(str_detect(key, "spill"), "spillover", "consequent"), 
         type = if_else(str_detect(key, "^log"), "log", 
                        if_else(str_detect(key, "^inv"), "inverse", "original")))

ggplot(data = dplot) +
  geom_histogram(mapping = aes(x = value), bins = 100) + 
  facet_grid(region ~ type)
```

As can be seen, the log transformation helps quite a bit in achieving approximate normality. Especially the `spillover` variable otherwise exhibits clearly a number of extreme values such as RTs above 5 seconds (consequent = `r round(mean(drp_r$consequent > 5), 2)`, spillover = `r round(mean(drp_r$spillover > 5), 2)`), above 3 seconds (consequent = `r round(mean(drp_r$consequent > 3), 2)`, spillover = `r round(mean(drp_r$spillover > 3), 2)`) or above 2.5 seconds (consequent = `r round(mean(drp_r$consequent > 2.5), 2)`, spillover = `r round(mean(drp_r$spillover > 2.5), 2)`).

# Consequent

Note that the maximal model for models including `Fit` have random slopes for `Fit` for both random effects grouping factors (i.e., participant `P.s` and `Item`). For models including the continuous fit measure `Accepts` we have by-participant random slopes for `Accepts`, but by-item random slopes for `Fit` (as `Accepts` has only two different values for each `Item`,  using the binary `Fit` variable seems more reasonable.)

## With Fit

```{r}
m_fp_con_1 <- mixed(log_con ~ Fit + (Fit|P.s) + (Fit| Item), drp_r)
summary(m_fp_con_1)
```

Due to the large correlation between the by-participants intercept and slope we refit the model without the correlation between the by-participant random effect parameters.

```{r}
m_fp_con_2 <- mixed(log_con ~ Fit + (Fit||P.s) + (Fit| Item), drp_r, expand_re = TRUE)
summary(m_fp_con_2)
```
Now the by-participant random slope for `Fit` shows no variability so we consequently refit the model without that parameter.

```{r}
m_fp_con_3 <- mixed(log_con ~ Fit + (1|P.s) + (Fit| Item), drp_r)
summary(m_fp_con_3)
```

This model seems reasonable and only provides a minimally worse account than the first model.

```{r}
anova(m_fp_con_1, m_fp_con_3)
```

However, the QQ-plot of both models show some misfit for long RTs, but nothing too dramatic. Significant outliers (i.e., observations with absolute standardized/normalized residuals greater than the .995 quantile of the standard normal distribution) are identified by the number of the participant providing this response.

```{r, fig.width=8, fig.height=4}
qqp_m1 <- qqmath(m_fp_con_1$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_m2 <- qqmath(m_fp_con_3$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_m1, qqp_m2, ncol = 2)
```


Nevertheless, the effect of `Fit` is significant.

```{r}
nice(m_fp_con_1)
nice(m_fp_con_3)
```

## With Accepts (log-transformed)

```{r}
m_fp_con_11 <- mixed(log_con ~ log_acc + (log_acc|P.s) + (Fit| Item), drp_r)
summary(m_fp_con_11)
```

As for the binary variable we see a suspcisouly high correlation between the by-participant random-effects parameters, but here also for the by-item random-effects parameters.


```{r}
m_fp_con_12 <- mixed(log_con ~ log_acc + (log_acc||P.s) + (Fit||Item), drp_r, expand_re = TRUE)
summary(m_fp_con_12)
```

```{r}
anova(m_fp_con_11, m_fp_con_12)
```

The difference in goodness of fit is more noticeable and approaches significance. We therefore inspect the QQ-plots of both models. As they both look identical we prefer the model with correlations due to the slightly better fit.

```{r, fig.width=8, fig.height=4}
qqp_11 <- qqmath(m_fp_con_11$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_12 <- qqmath(m_fp_con_12$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_11, qqp_12, ncol = 2)
```

In any case, both models show a significant effect of `log_acc`.

```{r}
nice(m_fp_con_11)
nice(m_fp_con_12)
```

## With Accepts (non-transformed)

```{r}
m_fp_con_21 <- mixed(log_con ~ acc_z + (acc_z|P.s) + (Fit| Item), drp_r)
summary(m_fp_con_21)
```

We see a suspcisouly high correlation between the by-item random-effects parameters, consequently we remove those.


```{r}
m_fp_con_22 <- mixed(log_con ~ acc_z + (acc_z|P.s) + (Fit||Item), drp_r, expand_re = TRUE)
summary(m_fp_con_22)
```

```{r}
anova(m_fp_con_21, m_fp_con_22)
```

The difference in goodness of fit is rather small.  We therefore inspect the QQ-plots of both models. As they both look identical we prefer the model with correlations due to the slightly better fit.

```{r, fig.width=8, fig.height=4}
qqp_21 <- qqmath(m_fp_con_21$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_22 <- qqmath(m_fp_con_22$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_21, qqp_22, ncol = 2)
```

In any case, both models show a significant effect of `Accepts` (z-transformed to `acc_z`).

```{r}
nice(m_fp_con_21)
nice(m_fp_con_22)
```

## Sanity Checks

To make sure our decision to use the logged RTs was justified we also fit the maximal model once with the other two DVs.

```{r}
m_fp_con_91 <- mixed(consequent ~ acc_z + (acc_z|P.s) + (Fit| Item), drp_r, progress = FALSE)
m_fp_con_92 <- mixed(inv_con ~ acc_z + (acc_z|P.s) + (Fit| Item), drp_r, progress = FALSE)
```

```{r, fig.width=8, fig.height=4}
qqp_91 <- qqmath(m_fp_con_91$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_92 <- qqmath(m_fp_con_92$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_91, qqp_92, ncol = 2)
```

From their QQ-plots it is obvious that the impact of the outliers is a lot more dramatic for the other two DVs than for the models with logged RTs. This is especially true for the untransformed RTs. Consequently, the logged RTs seems to be the most reasonable DV.

## Summary

```{r}
anova(m_fp_con_1, m_fp_con_2, m_fp_con_3, m_fp_con_11, m_fp_con_12, m_fp_con_21, m_fp_con_22)
```

The overall best models appear to be with `log_acc` as independent variable. All models show some problematic behavior in the QQ-plot for long RTs, but nothing too dramatic and not unexpected with RTs.  

In any case, the effect of the explanatory goodness of fit was significant for all models (the only exception was the one model with untransformed RTs, but this is most likely due to the presence of outliers). Participants were slower for items with low fit as seen below.

```{r, fig.width=8, fig.height=4}
drd_a2 <- drp_r %>% 
  group_by(P.s, Fit) %>% 
  summarise(log_con = mean(log_con))

drp_r$Fit2 <- jitter(as.numeric(drp_r$Fit))
drd_a2$Fit2 <- jitter(as.numeric(drd_a2$Fit))

coef_mfp_11 <- coef(m_fp_con_11$full_model)
coef_mfp_22 <- coef(m_fp_con_22$full_model)
coef_mfp_22 <- data.frame(intercept = coef_mfp_22$P.s$`(Intercept)`,
                          slope = coef_mfp_22$P.s$acc_z + coef_mfp_22$P.s$re1.acc_z)

pres_rp_a <- ggplot(data = drp_r, mapping = aes(x = Fit, y = log_con)) +
  geom_boxplot(outlier.color =  NA) +
  geom_line(data = drd_a2, aes(x=Fit2, y=log_con, group = P.s), alpha = 0.5) +
  geom_point(data = drd_a2, aes(x=Fit2, y=log_con, group = P.s), alpha = 0.5) +
  stat_summary(fun.y = "mean", colour = "red", size = 2, geom = "point")

pres_rp_b <- ggplot(data = drp_r, mapping = aes(x = log_acc, y = log_con)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef_mfp_11$P.s$`(Intercept)`, slope = coef_mfp_11$P.s$log_acc, alpha = 0.5)
pres_rp_c <- ggplot(data = drp_r, mapping = aes(x = acc_z, y = log_con)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef_mfp_22$intercept, slope = coef_mfp_22$slope, alpha = 0.5)

grid.arrange(pres_rp_a, pres_rp_b, pres_rp_c, ncol = 3)
```

# Spillover

## With Fit

```{r}
m_fp_spill_f_1 <- mixed(log_spill ~ Fit + (Fit|P.s) + (Fit| Item), drp_r)
summary(m_fp_spill_f_1)
```

Due to the large correlation between the by-participants and by-item random-effects parameters we refit the model without the correlations.

```{r}
m_fp_spill_f_2 <- mixed(log_spill ~ Fit + (Fit||P.s) + (Fit||Item), drp_r, expand_re = TRUE)
summary(m_fp_spill_f_2)
```
Now both random slopes are estimated to be zero, we consequently refit the model without those parameters.

```{r}
m_fp_spill_f_3 <- mixed(log_spill ~ Fit + (1|P.s) + (1| Item), drp_r)
summary(m_fp_spill_f_3)
```

This model seems reasonable and only provides a minimally worse account than the first model.

```{r}
anova(m_fp_spill_f_1, m_fp_spill_f_3)
```

The QQ-plot of both models show only minimal misfit. Interestingly, participant 10 seems somewhat problematic.

```{r, fig.width=8, fig.height=4}
qqp_m1 <- qqmath(m_fp_spill_f_1$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_m2 <- qqmath(m_fp_spill_f_3$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_m1, qqp_m2, ncol = 2)
```


The effect of Fit is only approaching significance in both cases.

```{r}
nice(m_fp_spill_f_1)
nice(m_fp_spill_f_3)
```

## With Accepts (log-transformed)

```{r}
m_fp_spill_al_1 <- mixed(log_spill ~ log_acc + (log_acc|P.s) + (Fit| Item), drp_r)
summary(m_fp_spill_al_1)
```

This model shows gives some convergence warnings and one correlation estimate is `NaN`. Additional the other correlation is at the boundary of the parameter space. We refit the model without the correlations.

```{r}
m_fp_spill_al_2 <- mixed(log_spill ~ log_acc + (log_acc||P.s) + (Fit||Item), drp_r, expand_re = TRUE)
summary(m_fp_spill_al_2)
```
The by-item random intercept is zero hence we refit the model without the random slope parameter (one cannot easily remove the random intercept and keep a categorical random slope). 

```{r}
m_fp_spill_al_3 <- mixed(log_spill ~ log_acc + (log_acc||P.s) + (1| Item), drp_r, expand_re = TRUE)
summary(m_fp_spill_al_3)
```

Overall the best model appears to be the last model.

```{r}
anova(m_fp_spill_al_1,m_fp_spill_al_2, m_fp_spill_al_3)
```

The QQ-plot of both models is essentially identical to the fit models with participant 10 showing some problematic behavior.

```{r, fig.width=8, fig.height=4}
qqp_m1 <- qqmath(m_fp_spill_al_1$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_m2 <- qqmath(m_fp_spill_al_3$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_m1, qqp_m2, ncol = 2)
```


The effect of `log_acc` is significant for the optimal model and almost significant for the maximal model.

```{r}
nice(m_fp_spill_al_1)
nice(m_fp_spill_al_3)
```

## With Accepts (non-transformed)

```{r}
m_fp_spill_a_1 <- mixed(log_spill ~ acc_z + (acc_z|P.s) + (Fit| Item), drp_r)
summary(m_fp_spill_a_1)
```

Again we observe large correlations between the by-participants and by-item random-effects parameters and we refit the model without the correlations.

```{r}
m_fp_spill_a_2 <- mixed(log_spill ~ acc_z + (acc_z||P.s) + (Fit||Item), drp_r, expand_re = TRUE)
summary(m_fp_spill_a_2)
```
And both random slopes are estimated to be zero, we consequently refit the model without those parameters.

```{r}
m_fp_spill_a_3 <- mixed(log_spill ~ acc_z + (1|P.s) + (1| Item), drp_r)
summary(m_fp_spill_a_3)
```

This model seems reasonable and only provides a minimally worse account than the first model.

```{r}
anova(m_fp_spill_a_1, m_fp_spill_a_3)
```

The QQ-plot of both models is essentially identical to the fit models with participant 10 showing some problematic behavior.

```{r, fig.width=8, fig.height=4}
qqp_m1 <- qqmath(m_fp_spill_a_1$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
qqp_m2 <- qqmath(m_fp_spill_a_3$full_model, id = 0.01, idLabels = as.character(drp_r$P.s))
grid.arrange(qqp_m1, qqp_m2, ncol = 2)
```


The effect of `acc_z` now is almost significant for the maximal model (which appears to be overparameterized) and significant for the random-intercepts only model.

```{r}
nice(m_fp_spill_a_1)
nice(m_fp_spill_a_3)
```

## Models Without Participant 10

Finally we also remove the problematic participant 10 and refit the optimal models.

```{r}
drp_r2 <- droplevels(drp_r[!(drp_r$P.s %in% c("10")),])
m_fp_spill_f_3b <- mixed(log_spill ~ Fit + (1|P.s) + (1| Item), drp_r2, progress = FALSE)
m_fp_spill_al_3b <- mixed(log_spill ~ log_acc + (log_acc||P.s) + (1| Item), drp_r2, expand_re = TRUE, progress = FALSE)
m_fp_spill_a_3b <- mixed(log_spill ~ acc_z + (1|P.s) + (1| Item), drp_r2, progress = FALSE)
nice(m_fp_spill_f_3b)
nice(m_fp_spill_al_3b)
nice(m_fp_spill_a_3b)
```

```{r, fig.width=12, fig.height=4}
qqp_m1 <- qqmath(m_fp_spill_f_3b$full_model, id = 0.01, idLabels = as.character(drp_r2$P.s))
qqp_m2 <- qqmath(m_fp_spill_al_3b$full_model, id = 0.01, idLabels = as.character(drp_r2$P.s))
qqp_m3 <- qqmath(m_fp_spill_a_3b$full_model, id = 0.01, idLabels = as.character(drp_r2$P.s))
grid.arrange(qqp_m1, qqp_m2, qqp_m3, ncol = 3)
```

This analysis shows that, whereas the effect of `Fit` is still not significant it clearly is for the other two model variants (with logged or untransformed `Accepts` as IV). In addition, the QQ-plots look even less problematic.

## Summary

```{r}
anova(m_fp_spill_f_1, m_fp_spill_f_3, m_fp_spill_al_1, m_fp_spill_al_3, m_fp_spill_a_1, m_fp_spill_a_3)
```

The overall best models again appear to be with `log_acc` as independent variable. Removing the participant that shows the most problematic behavior in the QQ-plot does not appear to change the results.  

In any case, the effect of the explanatory goodness of fit was significant for most models and clearly significant for all reasonable models. Participants were slower for items with low fit as seen below.

```{r, fig.width=8, fig.height=4}
drd_a2 <- drp_r %>% 
  group_by(P.s, Fit) %>% 
  summarise(log_spill = mean(log_spill))

drp_r$Fit2 <- jitter(as.numeric(drp_r$Fit))
drd_a2$Fit2 <- jitter(as.numeric(drd_a2$Fit))

coef_mfp_11 <- coef(m_fp_spill_al_3$full_model)
coef_mfp_11<- data.frame(intercept = coef_mfp_11$P.s$`(Intercept)`,
                          slope = coef_mfp_11$P.s$log_acc + coef_mfp_11$P.s$re1.log_acc)
coef_mfp_22 <- coef(m_fp_spill_a_3$full_model)

pres_rp_a <- ggplot(data = drp_r, mapping = aes(x = Fit, y = log_spill)) +
  geom_boxplot(outlier.color =  NA) +
  geom_line(data = drd_a2, aes(x=Fit2, y=log_spill, group = P.s), alpha = 0.5) +
  geom_point(data = drd_a2, aes(x=Fit2, y=log_spill, group = P.s), alpha = 0.5) +
  stat_summary(fun.y = "mean", colour = "red", size = 2, geom = "point")

pres_rp_b <- ggplot(data = drp_r, mapping = aes(x = log_acc, y = log_spill)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef_mfp_11$intercept, slope = coef_mfp_11$slope, alpha = 0.5)

pres_rp_c <- ggplot(data = drp_r, mapping = aes(x = acc_z, y = log_spill)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = coef_mfp_22$P.s$`(Intercept)`, slope = coef_mfp_22$P.s$acc_z, alpha = 0.5) 

grid.arrange(pres_rp_a, pres_rp_b, pres_rp_c, ncol = 3)
```
